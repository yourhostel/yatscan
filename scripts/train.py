# scripts/train.py

from transformers import VisionEncoderDecoderModel, TrOCRProcessor, Seq2SeqTrainer, Seq2SeqTrainingArguments
from transformers import default_data_collator, PreTrainedTokenizerFast
from torch.utils.data import Dataset as TorchBaseDataset
from sklearn.model_selection import train_test_split
from datasets import Dataset
from PIL import Image
import pandas as pd
import shutil
import torch
import os

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä–∞ –∑ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ JSON
tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer/tokenizer.json")
tokenizer.pad_token = "[PAD]"
tokenizer.unk_token = "[UNK]"
tokenizer.cls_token = "[CLS]"
tokenizer.sep_token = "[SEP]"
tokenizer.mask_token = "[MASK]"

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ TrOCRProcessor —ñ –±–µ—Ä–µ–º–æ —Ç—ñ–ª—å–∫–∏ –≤–∏—Ç—è–≥—É–≤–∞—á –∑–æ–±—Ä–∞–∂–µ–Ω—å
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-stage1")
feature_extractor = processor.feature_extractor

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å TrOCR —Ç–∞ –ø—ñ–¥–∫–ª—é—á–∞—î–º–æ —Å–≤—ñ–π —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-stage1")
model.config.decoder_start_token_id = tokenizer.cls_token_id or tokenizer.bos_token_id
model.config.pad_token_id = tokenizer.pad_token_id

# –û–Ω–æ–≤–ª—é—î–º–æ vocab_size –£–°–Æ–î–ò, –∫—É–¥–∏ —Ç—ñ–ª—å–∫–∏ –º–æ–∂–Ω–∞ (—â–æ–± TrOCR –Ω–µ –≥–µ–Ω–µ—Ä—É–≤–∞–≤ —Ç–æ–∫–µ–Ω–∏ –ø–æ–∑–∞ —Å–ª–æ–≤–Ω–∏–∫–æ–º)
new_vocab_size = len(tokenizer)
model.config.vocab_size = new_vocab_size
model.decoder.config.vocab_size = new_vocab_size
model.config.decoder.vocab_size = new_vocab_size
model.decoder.resize_token_embeddings(new_vocab_size)

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è CSV
df = pd.read_csv("dataset.csv")

# –†–æ–∑–±–∏–≤–∞—î–º–æ –¥–∞–Ω—ñ –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ —Ç–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ 80%/20%)
train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)

# üí° –î–∏–≤–∏–º–æ—Å—å –Ω–∞–π—á–∞—Å—Ç—ñ—à—ñ —Ç–µ–∫—Å—Ç–∏
# print(train_data["text"].value_counts().head(10))

# –°—Ç–≤–æ—Ä—é—î–º–æ HuggingFace-–¥–∞—Ç–∞—Å–µ—Ç–∏
train_dataset = Dataset.from_pandas(train_data)
eval_dataset = Dataset.from_pandas(val_data)

# –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥
def preprocess(example):
    image = Image.open(example["image"]).convert("RGB")
    pixel_values = feature_extractor(images=image, return_tensors="pt").pixel_values[0]

    labels = tokenizer(
        example["text"],
        padding="max_length",
        max_length=128,
        truncation=True
    ).input_ids
    labels = [l if l != tokenizer.pad_token_id else -100 for l in labels]

    return {
        "pixel_values": pixel_values,
        "labels": torch.tensor(labels, dtype=torch.long)
    }

# –∑–∞–¥–∞—î–º —Ñ–æ—Ä–º–∞—Ç ‚Äî —ñ –≤—ñ–Ω –∫–æ–Ω–≤–µ—Ä—Ç—É—î —Ü—ñ —Å–ø–∏—Å–∫–∏ –≤ —Ç–µ–Ω–∑–æ—Ä–∏:
# 1. –ó–±–∏—Ä–∞—î–º–æ –Ω–æ–≤—ñ –¥–∞–Ω—ñ
processed_train = [preprocess(example) for example in train_dataset]
processed_eval = [preprocess(example) for example in eval_dataset]

class TorchDataset(TorchBaseDataset):
    def __init__(self, data):
        self._data = data  # —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–Ω–∏–∫—ñ–≤ {"pixel_values": ..., "labels": ...}

    def __len__(self):
        return len(self._data)

    def __getitem__(self, idx):
        return self._data[idx]

train_dataset = TorchDataset(processed_train)
eval_dataset = TorchDataset(processed_eval)

print(type(train_dataset[0]["pixel_values"]))
print(train_dataset[0]["pixel_values"].shape)

# –Ø–∫—â–æ –º–æ–¥–µ–ª—å –≤–∂–µ —î ‚Äî –≤–∏–¥–∞–ª—è—î–º–æ
if os.path.exists("./model"):
    shutil.rmtree("./model")

# –ê—Ä–≥—É–º–µ–Ω—Ç–∏ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
training_args = Seq2SeqTrainingArguments(
    output_dir="./model",                         # –ö–∞—Ç–∞–ª–æ–≥ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
    per_device_train_batch_size=1,                # –†–æ–∑–º—ñ—Ä –±–∞—Ç—á—É –Ω–∞ –∫–æ–∂–µ–Ω GPU/CPU
    num_train_epochs=10,                          # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö (–ø–æ–≤–Ω–∏—Ö –ø—Ä–æ—Ö–æ–¥—ñ–≤ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É)
    learning_rate=5e-5,                           # –ü–æ—á–∞—Ç–∫–æ–≤–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è
    warmup_steps=10,                              # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫—Ä–æ–∫—ñ–≤ —Ä–æ–∑—ñ–≥—Ä—ñ–≤—É (–±–µ–∑ –ø–æ–≤–Ω–æ–≥–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞)
    weight_decay=0.01,                            # L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è (–¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω—è)
    logging_dir="./logs",                         # –ö–∞—Ç–∞–ª–æ–≥ –¥–ª—è TensorBoard-–ª–æ–≥—ñ–≤
    logging_steps=10,                             # –Ø–∫ —á–∞—Å—Ç–æ –ª–æ–≥—É–≤–∞—Ç–∏ (—É –∫—Ä–æ–∫–∞—Ö)
    logging_first_step=True,                      # –õ–æ–≥—É–≤–∞—Ç–∏ –≤–∂–µ –Ω–∞ –ø–µ—Ä—à–æ–º—É –∫—Ä–æ—Ü—ñ
    save_strategy="epoch",                        # –Ø–∫ —á–∞—Å—Ç–æ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ –º–æ–¥–µ–ª—å ‚Äî –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ—ó –µ–ø–æ—Ö–∏
    save_total_limit=2,                           # –ú–∞–∫—Å–∏–º—É–º –∑–±–µ—Ä–µ–∂–µ–Ω–∏—Ö —á–µ–∫–ø–æ–π–Ω—Ç—ñ–≤ (—Å—Ç–∞—Ä—ñ –≤–∏–¥–∞–ª—è—é—Ç—å—Å—è)
    report_to="tensorboard",                      # –ê–∫—Ç–∏–≤–∞—Ü—ñ—è –ª–æ–≥—É–≤–∞–Ω–Ω—è –≤ TensorBoard
    predict_with_generate=True,                   # –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –ø—Ä–∏ –æ—Ü—ñ–Ω—Ü—ñ
    generation_max_length=128,                    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
    evaluation_strategy="epoch",                  # –ö–æ–ª–∏ –ø—Ä–æ–≤–æ–¥–∏—Ç–∏ –æ—Ü—ñ–Ω–∫—É ‚Äî –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ—ó –µ–ø–æ—Ö–∏
    fp16=torch.cuda.is_available()                # –£–≤—ñ–º–∫–Ω—É—Ç–∏ FP16, —è–∫—â–æ —î –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ GPU
)

# –¢—Ä–µ–Ω–µ—Ä ‚Äî –æ–±–≥–æ—Ä—Ç–∫–∞, —è–∫–∞ –∑–∞–ø—É—Å–∫–∞—î –Ω–∞–≤—á–∞–Ω–Ω—è, –æ—Ü—ñ–Ω–∫—É —ñ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
trainer = Seq2SeqTrainer(
    model=model,                                  # –ù–∞—à–∞ –º–æ–¥–µ–ª—å TrOCR
    args=training_args,                           # –ê—Ä–≥—É–º–µ–Ω—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è (–≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏)
    train_dataset=train_dataset,                  # –ù–∞–≤—á–∞–ª—å–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç
    eval_dataset=eval_dataset,                    # –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç ‚Äî –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ—ó –µ–ø–æ—Ö–∏
    tokenizer=tokenizer,                          # –ö–∞—Å—Ç–æ–º–Ω–∏–π —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä (–í–ê–ñ–õ–ò–í–û!)
    data_collator=default_data_collator           # –ö–æ–ª–ª–∞—Ç–æ—Ä ‚Äî —â–æ–± –∑—ñ–±—Ä–∞—Ç–∏ –±–∞—Ç—á—ñ (–¥–æ–¥–∞—î –ø–∞–¥–¥—ñ–Ω–≥ —ñ —Ç.–¥.)
)

# –°—Ç–∞—Ä—Ç —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
trainer.train()


# ‚ú® –ì–µ–Ω–µ—Ä—É—î–º–æ –ø—Ä–∏–∫–ª–∞–¥–∏ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –ø—ñ—Å–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
print("\n=== –ü–†–ò–ö–õ–ê–î –ü–ï–†–ï–î–ë–ê–ß–ï–ù–ù–Ø ===")

# –ë–µ—Ä–µ–º–æ –æ–¥–∏–Ω –ø—Ä–∏–∫–ª–∞–¥ —ñ–∑ eval
sample = eval_dataset[0]
input_tensor = sample["pixel_values"].unsqueeze(0)  # [1, 3, 384, 384]

# –ì–µ–Ω–µ—Ä—É—î–º–æ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
with torch.no_grad():
    generated_ids = model.generate(input_tensor.to(model.device), max_length=128)

# –î–µ–∫–æ–¥—É—î–º–æ
decoded_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

# –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ç–µ–∫—Å—Ç
original_text = tokenizer.decode([id for id in sample["labels"] if id != -100], skip_special_tokens=True)

print(f"üñºÔ∏è Original : {original_text}")
print(f"ü§ñ Predicted: {decoded_text}")

# –û–Ω–æ–≤–ª—é—î–º–æ vocab_size –Ω–∞ —Ä–æ–∑–º—ñ—Ä –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä–∞
model.config.vocab_size = tokenizer.vocab_size

print("Token count:", len(tokenizer))
print("Model vocab size:", model.config.vocab_size)

# –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
model.save_pretrained("./model")
tokenizer.save_pretrained("./model")